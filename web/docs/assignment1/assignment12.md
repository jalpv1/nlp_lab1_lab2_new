# Task N2. Cross-Entropy and Language Modeling
## English Language
Number of unique words in texten1 8696
### English datasets
Data: 221098
Train data: 161098
Test data: 20000
Heldout data: 40000
### Compute parameters from the heldout data 
Smoothing params for heldout  3.1592330034284245e-07, 4.298408473449929e-06, 0.024478297587248517, 0.9755170880809776
### Compute parameters from the training data 
Smoothing params for training  1.0229764686886036e-07, 1.2399663412920048e-06, 0.012254713918027809, 0.987743943817984
### Experiments  
| Percent from difference  |         Entropy          |
| ------------------------ | ------------------------ |
| 10                       | 216.99416275815975       |
| 20                       | 213.6908849652181        |
| 30                       | 210.38437578304593       |
| 40                       | 207.0746382390176        |
| 50                       | 203.76167535274024       |
| 60                       | 200.44549013579598       |
| 70                       | 197.12608559182203       |
| 80                       | 193.80346471637517       |
| 90                       | 190.47763049675362       |
| 95                       | 188.81350931419084       |
| 99                       | 187.48163475224342       |

----------------------------------------------------------------------------------------
|         Percent          |         Entropy          |
| ------------------------ | ------------------------ |
| 0                        | 220.29420612656236       |
| 10                       | 216.99416275815975       |
| 20                       | 213.6908849652181        |
| 30                       | 210.38437578304593       |
| 40                       | 207.0746382390176        |
| 50                       | 203.76167535274024       |
| 60                       | 200.44549013579598       |
| 70                       | 197.12608559182203       |
| 80                       | 193.80346471637517       |
| 90                       | 190.47763049675362       |

